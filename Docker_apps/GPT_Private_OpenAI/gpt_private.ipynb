{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba560b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instala las librerías necesarias\n",
    "!pip install streamlit torch PyPDF2 langchain transformers chromadb streamlit-extras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af28e7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import torch\n",
    "import os\n",
    "from streamlit_extras.add_vertical_space import add_vertical_space\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from transformers import (\n",
    "    AutoModelForQuestionAnswering,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoModelForCausalLM,\n",
    "    pipeline\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57513ee9",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Forzar uso de CPU\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89babc4e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Cargar modelo para QA\n",
    "def load_qa_model():\n",
    "    qa_model_name = \"deepset/roberta-base-squad2\"\n",
    "    qa_tokenizer = AutoTokenizer.from_pretrained(qa_model_name)\n",
    "    qa_model = AutoModelForQuestionAnswering.from_pretrained(qa_model_name).to(device)\n",
    "    return qa_model, qa_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da2d9cb",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Cargar modelo para Resumen\n",
    "def load_summarization_model():\n",
    "    summarization_model_name = \"mrm8488/bert2bert_shared-spanish-finetuned-summarization\"\n",
    "    summarization_tokenizer = AutoTokenizer.from_pretrained(summarization_model_name)\n",
    "    summarization_model = AutoModelForSeq2SeqLM.from_pretrained(summarization_model_name).to(device)\n",
    "    return summarization_model, summarization_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e581e213",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Cargar modelo para Búsqueda Semántica\n",
    "def load_feature_extraction_pipeline():\n",
    "    embedding_pipeline = pipeline(\"feature-extraction\", model=\"sentence-transformers/LaBSE\")\n",
    "    return embedding_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47dd28b6",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Cargar modelo para Generación de Texto\n",
    "def load_text_generation_model():\n",
    "    text_generation_model_name = \"mrm8488/GPT2-finetuned-spanish\"\n",
    "    text_generation_tokenizer = AutoTokenizer.from_pretrained(text_generation_model_name)\n",
    "    text_generation_model = AutoModelForCausalLM.from_pretrained(text_generation_model_name).to(device)\n",
    "    return text_generation_model, text_generation_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d23a974",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Cargar los modelos\n",
    "qa_model, qa_tokenizer = load_qa_model()\n",
    "summarization_model, summarization_tokenizer = load_summarization_model()\n",
    "embedding_pipeline = load_feature_extraction_pipeline()\n",
    "text_generation_model, text_generation_tokenizer = load_text_generation_model()\n",
    "\n",
    "def generate_qa_response(question, context):\n",
    "    inputs = qa_tokenizer(question, context, return_tensors=\"pt\").to(device)\n",
    "    outputs = qa_model(**inputs)\n",
    "    start_index = torch.argmax(outputs.start_logits)\n",
    "    end_index = torch.argmax(outputs.end_logits)\n",
    "    answer = qa_tokenizer.decode(inputs[\"input_ids\"][0][start_index:end_index+1])\n",
    "    return answer\n",
    "\n",
    "def generate_summary(text):\n",
    "    inputs = summarization_tokenizer(text, return_tensors=\"pt\", max_length=512).to(device)\n",
    "    outputs = summarization_model.generate(**inputs)\n",
    "    summary = summarization_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return summary\n",
    "\n",
    "def generate_text_response(prompt):\n",
    "    inputs = text_generation_tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    outputs = text_generation_model.generate(**inputs, max_new_tokens=256)\n",
    "    response = text_generation_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "def main():\n",
    "    st.title(\"PMV-GPT-Local en Español\")\n",
    "    task = st.selectbox(\n",
    "        \"Selecciona la tarea que deseas realizar:\",\n",
    "        (\"Pregunta y Respuesta\", \"Resumen\", \"Búsqueda Semántica\", \"Generación de Texto\")\n",
    "    )\n",
    "\n",
    "    if task == \"Pregunta y Respuesta\":\n",
    "        context = st.text_area(\"Contexto:\")\n",
    "        question = st.text_input(\"Pregunta:\")\n",
    "        if st.button(\"Generar Respuesta\"):\n",
    "            response = generate_qa_response(question, context)\n",
    "            st.write(response)\n",
    "\n",
    "    elif task == \"Resumen\":\n",
    "        text = st.text_area(\"Texto a resumir:\")\n",
    "        if st.button(\"Generar Resumen\"):\n",
    "            summary = generate_summary(text)\n",
    "            st.write(summary)\n",
    "\n",
    "    elif task == \"Búsqueda Semántica\":\n",
    "        text = st.text_area(\"Texto para embeddings:\")\n",
    "        if st.button(\"Generar Embeddings\"):\n",
    "            embeddings = embedding_pipeline(text)\n",
    "            st.write(embeddings)\n",
    "\n",
    "    elif task == \"Generación de Texto\":\n",
    "        prompt = st.text_input(\"Introduce un prompt para generar texto:\")\n",
    "        if st.button(\"Generar Texto\"):\n",
    "            response = generate_text_response(prompt)\n",
    "            st.write(response)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec8a481",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Sidebar content\n",
    "with st.sidebar:\n",
    "    st.title('PMV-GPT-Local')\n",
    "    st.markdown('''\n",
    "    ## Acerca de este MVP:\n",
    "    - Esta es una aplicación de lectura de PDF\n",
    "    - Se pretende facilitar el aterrizaje de nuevas incorporaciones\n",
    "    - Usa un LLM local para garantizar la privacidad de la información\n",
    "    ''')\n",
    "    add_vertical_space(5)\n",
    "    st.write('MVP 1.0 realizado por LNS, ENE-2025')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950ef809",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Procesamiento de PDFs\n",
    "def process_pdf():\n",
    "    st.header('Consultor de Documentación Interna')\n",
    "\n",
    "    # Subir un archivo PDF\n",
    "    pdf = st.file_uploader('Sube un PDF con la documentación', type='pdf')\n",
    "\n",
    "    if pdf is not None:\n",
    "        pdf_reader = PdfReader(pdf)\n",
    "\n",
    "        # Extraer texto del PDF\n",
    "        text = ''\n",
    "        for page in pdf_reader.pages:\n",
    "            text += page.extract_text()\n",
    "\n",
    "        # Dividir texto en fragmentos\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=1000,\n",
    "            chunk_overlap=200,\n",
    "            length_function=len\n",
    "        )\n",
    "        chunks = text_splitter.split_text(text=text)\n",
    "\n",
    "        # Crear nombre para el índice\n",
    "        store_name = pdf.name[:-4]\n",
    "        index_path = f'{store_name}_index'\n",
    "\n",
    "        try:\n",
    "            # Verificar si ya existe un índice guardado\n",
    "            if os.path.exists(index_path):\n",
    "                VectorStore = Chroma.load_local(\n",
    "                    index_path, \n",
    "                    allow_dangerous_deserialization=True\n",
    "                )\n",
    "                st.write('Embeddings cargados desde el disco.')\n",
    "            else:\n",
    "                # Calcular embeddings y crear un índice Chroma\n",
    "                embeddings = chunks  # Simplificado para GGML\n",
    "                VectorStore = Chroma.from_texts(chunks, embeddings)\n",
    "\n",
    "                # Guardar el índice Chroma\n",
    "                VectorStore.save_local(index_path)\n",
    "                st.write('Embeddings calculados y guardados.')\n",
    "\n",
    "            # Aceptar preguntas del usuario\n",
    "            query = st.text_input('Pregunta sobre este documento PDF:')\n",
    "            if query:\n",
    "                docs = VectorStore.similarity_search(query=query, k=3)\n",
    "                response = generate_text_response(query)\n",
    "                st.write(response)\n",
    "\n",
    "        except Exception as e:\n",
    "            st.error(f\"Error al cargar o guardar el índice: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b087559a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    process_pdf()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
